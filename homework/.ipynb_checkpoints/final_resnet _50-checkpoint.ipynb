{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from resnet_builder import resnet\n",
    "import keras\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import  img_to_array, load_img\n",
    "from PIL import Image\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (224, 224)\n",
    "nbofdata=700\n",
    "base_path = r'ml100-03-final/image_data/train/'\n",
    "layers_of_folders=0\n",
    "folder_list=[]    \n",
    "labels=['daisy','dandelion', 'rose', 'sunflower', 'tulip']\n",
    "\n",
    "if base_path :\n",
    "    folder_layers=[]\n",
    "    files = os.scandir(base_path)\n",
    "    #  Get the 1st layer of folder\n",
    "    first_folder = []\n",
    "    first_folder_kind = []\n",
    "    for entry in files:\n",
    "        if entry.is_dir():\n",
    "            first_folder.append(entry.path)\n",
    "            first_folder_kind.append(entry.name)\n",
    "    folder_layers.append(first_folder_kind)\n",
    "    folder_list.append(first_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunflower  finished!\n",
      "tulip  finished!\n",
      "dandelion  finished!\n",
      "daisy  finished!\n",
      "rose  finished!\n"
     ]
    }
   ],
   "source": [
    "datanumber=nbofdata\n",
    "blob=[]\n",
    "blob_nparray=[]\n",
    "image_data=[]\n",
    "conc = 0\n",
    "labels_dict={}\n",
    "fnamelist = {}\n",
    "for entry1 in folder_list[layers_of_folders - 1]:\n",
    "    blob = []\n",
    "    cellname = os.path.basename(os.path.dirname(entry1))  # extract cell name\n",
    "    # print(cellname)\n",
    "    concnames = os.path.basename(entry1)  # extract concentration\n",
    "    # print(concnames)\n",
    "    if concnames in labels:\n",
    "        labels_dict[conc] = concnames\n",
    "        fnamelist = glob.glob(os.path.join(entry1, '*.jpg'))\n",
    "        for filename in fnamelist[0:datanumber]:\n",
    "            im = Image.open(filename)\n",
    "            if im is not None:\n",
    "                if im.mode=='RGB':\n",
    "                    im=im.resize(size,Image.BILINEAR)\n",
    "                    imarray = np.array(im)\n",
    "                    blob.append(imarray)\n",
    "        ind = np.reshape(np.arange(1, len(blob) + 1), (-1, 1))\n",
    "        blob_nparray = np.reshape(np.asarray(blob), (len(blob), blob[1].size))\n",
    "        blob_nparray = np.hstack((blob_nparray, ind, conc * np.ones((len(blob), 1))))\n",
    "        image_data.append(np.asarray(blob_nparray, dtype=np.float32))\n",
    "        print(concnames+'  finished!')\n",
    "        conc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(labels)):\n",
    "    trytry=image_data[j][:]\n",
    "# Prepare data\n",
    "    LengthT = trytry.shape[0]\n",
    "    trytry_index = trytry[...,-2:-1]\n",
    "    trytry_label = trytry[...,-1:] #['Nega' for x in range(lengthN*4)] #Nega_data[...,-1:]\n",
    "    trytry = trytry[...,:-2]\n",
    "    \n",
    "    # Normalize image by subtracting mean image\n",
    "    trytry -= np.reshape(np.mean(trytry, axis=1), (-1,1))\n",
    "    # Reshape images\n",
    "    trytry = np.reshape(trytry, (trytry.shape[0],224,224,3))\n",
    "    \n",
    "#    # Rotate images\n",
    "#    for i in range(3):\n",
    "#        trytry[LengthT*(i+1):LengthT*(i+2)] = np.rot90(trytry[:LengthT], i+1, (1,2))\n",
    "    # Add channel dimension to fit in Conv2D\n",
    "    trytry = trytry.reshape(-1,224,224,3)\n",
    "    np.random.shuffle(trytry)\n",
    "    trytry_train_upto = round(trytry.shape[0] * 8 / 10)\n",
    "    trytry_test_upto = trytry.shape[0]\n",
    "    if j is 0:\n",
    "        train_data = trytry[:trytry_train_upto]\n",
    "        test_data = trytry[trytry_train_upto:trytry_test_upto]\n",
    "        train_label = trytry_label[:trytry_train_upto]\n",
    "        test_label = trytry_label[trytry_train_upto:trytry_test_upto]\n",
    "        \n",
    "    else:\n",
    "        train_data = np.concatenate((train_data, \n",
    "                                     trytry[:trytry_train_upto]), axis=0)\n",
    "        \n",
    "        test_data = np.concatenate((test_data, \n",
    "                                    trytry[trytry_train_upto:trytry_test_upto]), axis=0)\n",
    "        \n",
    "        train_label = np.concatenate((train_label, \n",
    "                                     trytry_label[:trytry_train_upto]), axis=0)\n",
    "        \n",
    "        \n",
    "        test_label = np.concatenate((test_label, \n",
    "                                    trytry_label[trytry_train_upto:trytry_test_upto]), axis=0)\n",
    "        \n",
    "test_label = keras.utils.to_categorical(test_label, num_classes=len(labels))\n",
    "train_label = keras.utils.to_categorical(train_label, num_classes=len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "temp = list(zip(train_data, train_label))\n",
    "\n",
    "random.shuffle(temp)\n",
    "\n",
    "train_data,train_label = zip(*temp)\n",
    "\n",
    "train_data=np.asarray(train_data)\n",
    "train_label=np.asarray(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150, 150, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 150, 150, 16) 448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 150, 150, 16) 64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 150, 150, 16) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 150, 150, 16) 272         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 150, 150, 16) 64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 150, 150, 16) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 150, 150, 16) 2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 150, 150, 16) 64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 150, 150, 16) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 150, 150, 64) 1088        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 150, 150, 64) 1088        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 150, 150, 64) 0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 150, 150, 64) 256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 150, 150, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 150, 150, 16) 1040        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 150, 150, 16) 64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 150, 150, 16) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 150, 150, 16) 2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 150, 150, 16) 64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 150, 150, 16) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 150, 150, 64) 1088        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 150, 150, 64) 0           add_1[0][0]                      \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 150, 150, 64) 256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 150, 150, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 150, 150, 16) 1040        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 150, 150, 16) 64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 150, 150, 16) 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 150, 150, 16) 2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 150, 150, 16) 64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 150, 150, 16) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 150, 150, 64) 1088        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 150, 150, 64) 0           add_2[0][0]                      \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 150, 150, 64) 256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 150, 150, 64) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 75, 75, 64)   4160        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 75, 75, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 75, 75, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 75, 75, 64)   36928       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 75, 75, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 75, 75, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 75, 75, 128)  8320        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 75, 75, 128)  8320        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 75, 75, 128)  0           conv2d_15[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 75, 75, 128)  512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 75, 75, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 75, 75, 64)   8256        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 75, 75, 64)   256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 75, 75, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 75, 75, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 75, 75, 64)   256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 75, 75, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 75, 75, 128)  8320        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 75, 75, 128)  0           add_4[0][0]                      \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 75, 75, 128)  512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 75, 75, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 75, 75, 64)   8256        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 75, 75, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 75, 75, 64)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 75, 75, 64)   36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 75, 75, 64)   256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 75, 75, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 75, 75, 128)  8320        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 75, 75, 128)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 75, 75, 128)  512         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 75, 75, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 38, 38, 128)  16512       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 38, 38, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 38, 38, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 38, 38, 128)  147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 38, 38, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 38, 38, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 38, 38, 256)  33024       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 38, 38, 256)  33024       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 38, 38, 256)  0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 38, 38, 256)  1024        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 38, 38, 256)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 38, 38, 128)  32896       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 38, 38, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 38, 38, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 38, 38, 128)  147584      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 38, 38, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 38, 38, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 38, 38, 256)  33024       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 38, 38, 256)  0           add_7[0][0]                      \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 38, 38, 256)  1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 38, 38, 256)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 38, 38, 128)  32896       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 38, 38, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 38, 38, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 38, 38, 128)  147584      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 38, 38, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 38, 38, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 38, 38, 256)  33024       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 38, 38, 256)  0           add_8[0][0]                      \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 38, 38, 256)  1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 38, 38, 256)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 4, 4, 256)    0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5)            20485       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 866,917\n",
      "Trainable params: 861,701\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.applications import ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2258 samples, validate on 565 samples\n",
      "Epoch 1/30\n",
      "2258/2258 [==============================] - 660s 292ms/step - loss: 1.9279 - accuracy: 0.4770 - val_loss: 68.8762 - val_accuracy: 0.2124\n",
      "Epoch 2/30\n",
      "2258/2258 [==============================] - 654s 290ms/step - loss: 1.6185 - accuracy: 0.5992 - val_loss: 22.1039 - val_accuracy: 0.2159\n",
      "Epoch 3/30\n",
      "2258/2258 [==============================] - 684s 303ms/step - loss: 1.4782 - accuracy: 0.6528 - val_loss: 11.8716 - val_accuracy: 0.2177\n",
      "Epoch 4/30\n",
      "2258/2258 [==============================] - 671s 297ms/step - loss: 1.3705 - accuracy: 0.6957 - val_loss: 3.4637 - val_accuracy: 0.4584\n",
      "Epoch 5/30\n",
      "2258/2258 [==============================] - 679s 301ms/step - loss: 1.2965 - accuracy: 0.7254 - val_loss: 3.5443 - val_accuracy: 0.4496\n",
      "Epoch 6/30\n",
      "2258/2258 [==============================] - 670s 297ms/step - loss: 1.2303 - accuracy: 0.7471 - val_loss: 2.1853 - val_accuracy: 0.5186\n",
      "Epoch 7/30\n",
      "2258/2258 [==============================] - 675s 299ms/step - loss: 1.1515 - accuracy: 0.7843 - val_loss: 1.8704 - val_accuracy: 0.5345\n",
      "Epoch 8/30\n",
      "2258/2258 [==============================] - 697s 309ms/step - loss: 1.0999 - accuracy: 0.8043 - val_loss: 1.6761 - val_accuracy: 0.6283\n",
      "Epoch 9/30\n",
      "2258/2258 [==============================] - 673s 298ms/step - loss: 1.0437 - accuracy: 0.8273 - val_loss: 1.6691 - val_accuracy: 0.5965\n",
      "Epoch 10/30\n",
      "2258/2258 [==============================] - 685s 303ms/step - loss: 0.9738 - accuracy: 0.8574 - val_loss: 1.5701 - val_accuracy: 0.6354\n",
      "Epoch 11/30\n",
      "2258/2258 [==============================] - 723s 320ms/step - loss: 0.9243 - accuracy: 0.8756 - val_loss: 1.4733 - val_accuracy: 0.6584\n",
      "Epoch 12/30\n",
      "2258/2258 [==============================] - 726s 322ms/step - loss: 0.8959 - accuracy: 0.8773 - val_loss: 1.4814 - val_accuracy: 0.6336\n",
      "Epoch 13/30\n",
      "2258/2258 [==============================] - 718s 318ms/step - loss: 0.8153 - accuracy: 0.9198 - val_loss: 1.7674 - val_accuracy: 0.5894\n",
      "Epoch 14/30\n",
      "2258/2258 [==============================] - 701s 311ms/step - loss: 0.7978 - accuracy: 0.9185 - val_loss: 1.5161 - val_accuracy: 0.6460\n",
      "Epoch 15/30\n",
      "2258/2258 [==============================] - 702s 311ms/step - loss: 0.7297 - accuracy: 0.9486 - val_loss: 1.4221 - val_accuracy: 0.6673\n",
      "Epoch 16/30\n",
      "2258/2258 [==============================] - 697s 309ms/step - loss: 0.7079 - accuracy: 0.9508 - val_loss: 1.5757 - val_accuracy: 0.6265\n",
      "Epoch 17/30\n",
      "2258/2258 [==============================] - 700s 310ms/step - loss: 0.6471 - accuracy: 0.9752 - val_loss: 1.7020 - val_accuracy: 0.6230\n",
      "Epoch 18/30\n",
      "2258/2258 [==============================] - 701s 310ms/step - loss: 0.6277 - accuracy: 0.9796 - val_loss: 1.7756 - val_accuracy: 0.5841\n",
      "Epoch 19/30\n",
      "2258/2258 [==============================] - 725s 321ms/step - loss: 0.6036 - accuracy: 0.9823 - val_loss: 1.4709 - val_accuracy: 0.6566\n",
      "Epoch 20/30\n",
      "2258/2258 [==============================] - 756s 335ms/step - loss: 0.5594 - accuracy: 0.9947 - val_loss: 1.3288 - val_accuracy: 0.6956\n",
      "Epoch 21/30\n",
      "2258/2258 [==============================] - 743s 329ms/step - loss: 0.5463 - accuracy: 0.9938 - val_loss: 1.8177 - val_accuracy: 0.6000\n",
      "Epoch 22/30\n",
      "2258/2258 [==============================] - 672s 298ms/step - loss: 0.5286 - accuracy: 0.9947 - val_loss: 1.6892 - val_accuracy: 0.6407\n",
      "Epoch 23/30\n",
      "2258/2258 [==============================] - 720s 319ms/step - loss: 0.5162 - accuracy: 0.9965 - val_loss: 1.8179 - val_accuracy: 0.6212\n",
      "Epoch 24/30\n",
      "2258/2258 [==============================] - 644s 285ms/step - loss: 0.4997 - accuracy: 0.9969 - val_loss: 1.7141 - val_accuracy: 0.6389\n",
      "Epoch 25/30\n",
      "2258/2258 [==============================] - 636s 282ms/step - loss: 0.4831 - accuracy: 0.9991 - val_loss: 1.9321 - val_accuracy: 0.5735\n",
      "Epoch 26/30\n",
      "2258/2258 [==============================] - 636s 282ms/step - loss: 0.4719 - accuracy: 1.0000 - val_loss: 1.7024 - val_accuracy: 0.6301\n",
      "Epoch 27/30\n",
      "2258/2258 [==============================] - 704s 312ms/step - loss: 0.4621 - accuracy: 0.9996 - val_loss: 1.7669 - val_accuracy: 0.6442\n",
      "Epoch 28/30\n",
      "2258/2258 [==============================] - 701s 310ms/step - loss: 0.4522 - accuracy: 1.0000 - val_loss: 1.7540 - val_accuracy: 0.6053\n",
      "Epoch 29/30\n",
      "2258/2258 [==============================] - 716s 317ms/step - loss: 0.4459 - accuracy: 0.9991 - val_loss: 1.8019 - val_accuracy: 0.6319\n",
      "Epoch 30/30\n",
      "2258/2258 [==============================] - 790s 350ms/step - loss: 0.4426 - accuracy: 0.9987 - val_loss: 1.5507 - val_accuracy: 0.6867\n",
      "Test loss: 1.5507460967629356\n",
      "Test accuracy: 0.6867256760597229\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model_ckpt = ModelCheckpoint(filepath=\"./tmp.h5\", \n",
    "                             monitor=\"val_loss\", \n",
    "                             save_best_only=True)\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 5 \n",
    "epochs = 30\n",
    "\n",
    "EStop = EarlyStopping(monitor='val_accuracy', min_delta=0, \n",
    "                      patience=6, verbose=1, mode='auto')\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_data, train_label,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(test_data, test_label),\n",
    "                    callbacks=[model_ckpt])\n",
    "score = model.evaluate(test_data, test_label, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"./tmp.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (224, 224)\n",
    "nbofdata=2000\n",
    "base_path = r'ml100-03-final/image_data/test/'\n",
    "layers_of_folders=0\n",
    "folder_list=[]    \n",
    "labels=['test']\n",
    "\n",
    "if base_path :\n",
    "    folder_layers=[]\n",
    "    files = os.scandir(base_path)\n",
    "    #  Get the 1st layer of folder\n",
    "    first_folder = []\n",
    "    first_folder_kind = []\n",
    "    for entry in files:\n",
    "        if entry.is_dir():\n",
    "            first_folder.append(entry.path)\n",
    "            first_folder_kind.append(entry.name)\n",
    "    folder_layers.append(first_folder_kind)\n",
    "    folder_list.append(first_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1]\n",
      " [   2]\n",
      " [   3]\n",
      " ...\n",
      " [1998]\n",
      " [1999]\n",
      " [2000]]\n",
      "[[1.160e+02 1.060e+02 9.000e+00 ... 3.000e+00 1.000e+00 0.000e+00]\n",
      " [1.980e+02 2.160e+02 2.240e+02 ... 5.300e+01 2.000e+00 0.000e+00]\n",
      " [3.500e+01 6.100e+01 3.600e+01 ... 3.300e+01 3.000e+00 0.000e+00]\n",
      " ...\n",
      " [1.080e+02 1.230e+02 3.000e+01 ... 0.000e+00 1.998e+03 0.000e+00]\n",
      " [1.390e+02 1.720e+02 1.990e+02 ... 2.000e+00 1.999e+03 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 1.000e+00 2.000e+03 0.000e+00]]\n",
      "test  finished!\n"
     ]
    }
   ],
   "source": [
    "datanumber=nbofdata\n",
    "blob=[]\n",
    "blob_nparray=[]\n",
    "image_data=[]\n",
    "conc = 0\n",
    "fc = 0\n",
    "labels_dict={}\n",
    "fn = {}\n",
    "for entry1 in folder_list[layers_of_folders - 1]:\n",
    "    blob = []\n",
    "    cellname = os.path.basename(os.path.dirname(entry1))  # extract cell name\n",
    "    # print(cellname)\n",
    "    concnames = os.path.basename(entry1)  # extract concentration\n",
    "    # print(concnames)\n",
    "    if concnames in labels:\n",
    "        labels_dict[conc] = concnames\n",
    "        fnamelist = glob.glob(os.path.join(entry1, '*.jpg'))\n",
    "        for filename in fnamelist[0:datanumber]:\n",
    "            im = Image.open(filename)\n",
    "            if im is not None:\n",
    "                if im.mode=='RGB':\n",
    "                    im=im.resize(size,Image.BILINEAR)\n",
    "                    imarray = np.array(im)\n",
    "                    blob.append(imarray)\n",
    "                    fn[fc] = filename\n",
    "                    fc += 1\n",
    "        ind = np.reshape(np.arange(1, len(blob) + 1), (-1, 1))\n",
    "        blob_nparray = np.reshape(np.asarray(blob), (len(blob), blob[1].size))\n",
    "        blob_nparray = np.hstack((blob_nparray, ind, conc * np.ones((len(blob), 1))))\n",
    "        image_data.append(np.asarray(blob_nparray, dtype=np.float32))\n",
    "        print(ind)\n",
    "        print(blob_nparray)\n",
    "        print(concnames+'  finished!')\n",
    "        conc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ml100-03-final/image_data/test/test/b38d1fef59f487bf8e702c5eab79880d.jpg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = {}\n",
    "for i in range(2000):\n",
    "    sp[i] = fn[i].split('/')\n",
    "    sp[i] = sp[i][4].split('.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b38d1fef59f487bf8e702c5eab79880d\n",
      "4cd32ea34f68e3b43c73341b8fb3d4c0\n",
      "aabbd368642e6843bb6f15a3afaa9ed0\n",
      "71469fb90f914a3639e7691ea2a64214\n",
      "6c6fc0a1bd638792e341c75949c76428\n",
      "fd2580a8f500b27baf6913759b29c003\n",
      "499790bb426abd7f293270ff2a357984\n",
      "d79a3d0a0e8120333f1ea82aaaad1dd0\n",
      "ff7eac29b6d7a33fbd8009677c3e9c58\n",
      "8ceefea6d56655f1689ae14a20c0f8be\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(sp[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = {}\n",
    "for i in range(2000):\n",
    "    ids[i] = sp[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(labels)):\n",
    "    trytry=image_data[j][:]\n",
    "# Prepare data\n",
    "    LengthT = trytry.shape[0]\n",
    "\n",
    "    trytry_index = trytry[...,-2:-1]\n",
    "\n",
    "    trytry_label = trytry[...,-1:] #['Nega' for x in range(lengthN*4)] #Nega_data[...,-1:]\n",
    "\n",
    "    trytry = trytry[...,:-2]\n",
    "\n",
    "    # Normalize image by subtracting mean image\n",
    "    trytry -= np.reshape(np.mean(trytry, axis=1), (-1,1))\n",
    "    # Reshape images\n",
    "    trytry = np.reshape(trytry, (trytry.shape[0],224,224,3))\n",
    "    \n",
    "#    # Rotate images\n",
    "#    for i in range(3):\n",
    "#        trytry[LengthT*(i+1):LengthT*(i+2)] = np.rot90(trytry[:LengthT], i+1, (1,2))\n",
    "    # Add channel dimension to fit in Conv2D\n",
    "    trytry = trytry.reshape(-1,224,224,3)\n",
    "    trytry_test_upto = trytry.shape[0]\n",
    "    if j is 0:\n",
    "        test_data = trytry[:trytry_test_upto]      \n",
    "    else:     \n",
    "        test_data = np.concatenate((test_data, \n",
    "                                    trytry[trytry_train_upto:trytry_test_upto]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 150, 150, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010461592\n",
      "4.37847e-05\n",
      "0.0008633345\n",
      "0.9975846\n",
      "0.00046215858\n",
      "0.99968076\n",
      "3.070935e-05\n",
      "9.125603e-05\n",
      "0.00014232114\n",
      "5.4965843e-05\n",
      "0.00017900941\n",
      "0.00575961\n",
      "0.9291866\n",
      "0.04864809\n",
      "0.016226752\n",
      "0.9934995\n",
      "0.005960266\n",
      "0.00020658711\n",
      "8.440831e-06\n",
      "0.00032539637\n",
      "0.005021618\n",
      "0.0021876404\n",
      "0.7611819\n",
      "0.08718855\n",
      "0.14442028\n",
      "0.0011487767\n",
      "0.0011555238\n",
      "0.032557555\n",
      "0.9600295\n",
      "0.005108641\n",
      "0.97358805\n",
      "0.00028051034\n",
      "0.004575447\n",
      "0.019069964\n",
      "0.0024859956\n",
      "0.04137893\n",
      "0.7631766\n",
      "0.015362793\n",
      "0.13142712\n",
      "0.04865443\n",
      "0.00013571208\n",
      "0.0005216331\n",
      "0.9929108\n",
      "0.005655849\n",
      "0.0007759446\n",
      "0.009264294\n",
      "0.9171813\n",
      "0.0095365895\n",
      "0.02227483\n",
      "0.04174301\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    for j in range(5):\n",
    "        print(predictions[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import array_to_img\n",
    "\n",
    "plt.imshow(array_to_img(test_data[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred = {}\n",
    "for i in range(2000):\n",
    "    for j in range(5):\n",
    "        if predictions[i][j] == np.max(predictions[i]):\n",
    "            if j == 0:\n",
    "                cnn_pred[i] = 3\n",
    "            if j == 1:\n",
    "                cnn_pred[i] = 4\n",
    "            if j == 2:\n",
    "                cnn_pred[i] = 1\n",
    "            if j == 3:\n",
    "                cnn_pred[i] = 0\n",
    "            if j == 4:\n",
    "                cnn_pred[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 3,\n",
       " 2: 1,\n",
       " 3: 3,\n",
       " 4: 1,\n",
       " 5: 0,\n",
       " 6: 3,\n",
       " 7: 4,\n",
       " 8: 1,\n",
       " 9: 4,\n",
       " 10: 1,\n",
       " 11: 2,\n",
       " 12: 4,\n",
       " 13: 1,\n",
       " 14: 0,\n",
       " 15: 2,\n",
       " 16: 1,\n",
       " 17: 1,\n",
       " 18: 1,\n",
       " 19: 0,\n",
       " 20: 2,\n",
       " 21: 0,\n",
       " 22: 4,\n",
       " 23: 4,\n",
       " 24: 3,\n",
       " 25: 4,\n",
       " 26: 1,\n",
       " 27: 4,\n",
       " 28: 0,\n",
       " 29: 4,\n",
       " 30: 4,\n",
       " 31: 4,\n",
       " 32: 4,\n",
       " 33: 3,\n",
       " 34: 4,\n",
       " 35: 1,\n",
       " 36: 4,\n",
       " 37: 4,\n",
       " 38: 1,\n",
       " 39: 1,\n",
       " 40: 2,\n",
       " 41: 2,\n",
       " 42: 1,\n",
       " 43: 4,\n",
       " 44: 1,\n",
       " 45: 3,\n",
       " 46: 1,\n",
       " 47: 0,\n",
       " 48: 1,\n",
       " 49: 1,\n",
       " 50: 3,\n",
       " 51: 2,\n",
       " 52: 0,\n",
       " 53: 0,\n",
       " 54: 1,\n",
       " 55: 0,\n",
       " 56: 4,\n",
       " 57: 4,\n",
       " 58: 1,\n",
       " 59: 2,\n",
       " 60: 0,\n",
       " 61: 0,\n",
       " 62: 0,\n",
       " 63: 1,\n",
       " 64: 4,\n",
       " 65: 4,\n",
       " 66: 1,\n",
       " 67: 3,\n",
       " 68: 1,\n",
       " 69: 0,\n",
       " 70: 1,\n",
       " 71: 0,\n",
       " 72: 1,\n",
       " 73: 0,\n",
       " 74: 0,\n",
       " 75: 2,\n",
       " 76: 2,\n",
       " 77: 1,\n",
       " 78: 1,\n",
       " 79: 2,\n",
       " 80: 1,\n",
       " 81: 0,\n",
       " 82: 0,\n",
       " 83: 3,\n",
       " 84: 3,\n",
       " 85: 1,\n",
       " 86: 4,\n",
       " 87: 1,\n",
       " 88: 2,\n",
       " 89: 1,\n",
       " 90: 3,\n",
       " 91: 2,\n",
       " 92: 4,\n",
       " 93: 0,\n",
       " 94: 3,\n",
       " 95: 2,\n",
       " 96: 4,\n",
       " 97: 3,\n",
       " 98: 0,\n",
       " 99: 1,\n",
       " 100: 3,\n",
       " 101: 2,\n",
       " 102: 0,\n",
       " 103: 2,\n",
       " 104: 1,\n",
       " 105: 1,\n",
       " 106: 3,\n",
       " 107: 0,\n",
       " 108: 2,\n",
       " 109: 3,\n",
       " 110: 3,\n",
       " 111: 4,\n",
       " 112: 1,\n",
       " 113: 1,\n",
       " 114: 0,\n",
       " 115: 4,\n",
       " 116: 1,\n",
       " 117: 1,\n",
       " 118: 3,\n",
       " 119: 1,\n",
       " 120: 1,\n",
       " 121: 1,\n",
       " 122: 4,\n",
       " 123: 1,\n",
       " 124: 4,\n",
       " 125: 4,\n",
       " 126: 0,\n",
       " 127: 0,\n",
       " 128: 3,\n",
       " 129: 1,\n",
       " 130: 4,\n",
       " 131: 2,\n",
       " 132: 3,\n",
       " 133: 2,\n",
       " 134: 1,\n",
       " 135: 2,\n",
       " 136: 3,\n",
       " 137: 4,\n",
       " 138: 2,\n",
       " 139: 4,\n",
       " 140: 0,\n",
       " 141: 2,\n",
       " 142: 0,\n",
       " 143: 1,\n",
       " 144: 1,\n",
       " 145: 0,\n",
       " 146: 4,\n",
       " 147: 1,\n",
       " 148: 0,\n",
       " 149: 1,\n",
       " 150: 1,\n",
       " 151: 4,\n",
       " 152: 3,\n",
       " 153: 0,\n",
       " 154: 1,\n",
       " 155: 1,\n",
       " 156: 0,\n",
       " 157: 2,\n",
       " 158: 4,\n",
       " 159: 3,\n",
       " 160: 1,\n",
       " 161: 1,\n",
       " 162: 1,\n",
       " 163: 1,\n",
       " 164: 2,\n",
       " 165: 0,\n",
       " 166: 3,\n",
       " 167: 1,\n",
       " 168: 1,\n",
       " 169: 3,\n",
       " 170: 1,\n",
       " 171: 0,\n",
       " 172: 1,\n",
       " 173: 4,\n",
       " 174: 4,\n",
       " 175: 0,\n",
       " 176: 2,\n",
       " 177: 1,\n",
       " 178: 4,\n",
       " 179: 4,\n",
       " 180: 4,\n",
       " 181: 4,\n",
       " 182: 1,\n",
       " 183: 3,\n",
       " 184: 0,\n",
       " 185: 4,\n",
       " 186: 1,\n",
       " 187: 1,\n",
       " 188: 0,\n",
       " 189: 4,\n",
       " 190: 4,\n",
       " 191: 3,\n",
       " 192: 4,\n",
       " 193: 4,\n",
       " 194: 1,\n",
       " 195: 0,\n",
       " 196: 0,\n",
       " 197: 4,\n",
       " 198: 0,\n",
       " 199: 0,\n",
       " 200: 3,\n",
       " 201: 1,\n",
       " 202: 0,\n",
       " 203: 1,\n",
       " 204: 2,\n",
       " 205: 1,\n",
       " 206: 3,\n",
       " 207: 1,\n",
       " 208: 4,\n",
       " 209: 3,\n",
       " 210: 2,\n",
       " 211: 1,\n",
       " 212: 1,\n",
       " 213: 3,\n",
       " 214: 3,\n",
       " 215: 3,\n",
       " 216: 4,\n",
       " 217: 1,\n",
       " 218: 3,\n",
       " 219: 0,\n",
       " 220: 2,\n",
       " 221: 2,\n",
       " 222: 4,\n",
       " 223: 0,\n",
       " 224: 0,\n",
       " 225: 4,\n",
       " 226: 3,\n",
       " 227: 1,\n",
       " 228: 2,\n",
       " 229: 3,\n",
       " 230: 0,\n",
       " 231: 1,\n",
       " 232: 0,\n",
       " 233: 0,\n",
       " 234: 2,\n",
       " 235: 1,\n",
       " 236: 3,\n",
       " 237: 4,\n",
       " 238: 3,\n",
       " 239: 1,\n",
       " 240: 1,\n",
       " 241: 1,\n",
       " 242: 3,\n",
       " 243: 4,\n",
       " 244: 1,\n",
       " 245: 1,\n",
       " 246: 1,\n",
       " 247: 2,\n",
       " 248: 3,\n",
       " 249: 4,\n",
       " 250: 3,\n",
       " 251: 2,\n",
       " 252: 4,\n",
       " 253: 4,\n",
       " 254: 2,\n",
       " 255: 4,\n",
       " 256: 1,\n",
       " 257: 1,\n",
       " 258: 4,\n",
       " 259: 2,\n",
       " 260: 2,\n",
       " 261: 0,\n",
       " 262: 2,\n",
       " 263: 3,\n",
       " 264: 0,\n",
       " 265: 1,\n",
       " 266: 4,\n",
       " 267: 1,\n",
       " 268: 4,\n",
       " 269: 2,\n",
       " 270: 2,\n",
       " 271: 1,\n",
       " 272: 4,\n",
       " 273: 1,\n",
       " 274: 2,\n",
       " 275: 2,\n",
       " 276: 1,\n",
       " 277: 0,\n",
       " 278: 4,\n",
       " 279: 3,\n",
       " 280: 0,\n",
       " 281: 4,\n",
       " 282: 1,\n",
       " 283: 4,\n",
       " 284: 3,\n",
       " 285: 2,\n",
       " 286: 3,\n",
       " 287: 1,\n",
       " 288: 4,\n",
       " 289: 0,\n",
       " 290: 4,\n",
       " 291: 3,\n",
       " 292: 0,\n",
       " 293: 2,\n",
       " 294: 2,\n",
       " 295: 4,\n",
       " 296: 2,\n",
       " 297: 3,\n",
       " 298: 1,\n",
       " 299: 0,\n",
       " 300: 1,\n",
       " 301: 1,\n",
       " 302: 1,\n",
       " 303: 3,\n",
       " 304: 4,\n",
       " 305: 4,\n",
       " 306: 0,\n",
       " 307: 4,\n",
       " 308: 3,\n",
       " 309: 1,\n",
       " 310: 4,\n",
       " 311: 3,\n",
       " 312: 1,\n",
       " 313: 3,\n",
       " 314: 2,\n",
       " 315: 3,\n",
       " 316: 4,\n",
       " 317: 2,\n",
       " 318: 1,\n",
       " 319: 1,\n",
       " 320: 4,\n",
       " 321: 0,\n",
       " 322: 2,\n",
       " 323: 4,\n",
       " 324: 4,\n",
       " 325: 3,\n",
       " 326: 1,\n",
       " 327: 1,\n",
       " 328: 3,\n",
       " 329: 0,\n",
       " 330: 0,\n",
       " 331: 3,\n",
       " 332: 1,\n",
       " 333: 4,\n",
       " 334: 1,\n",
       " 335: 3,\n",
       " 336: 4,\n",
       " 337: 1,\n",
       " 338: 2,\n",
       " 339: 0,\n",
       " 340: 1,\n",
       " 341: 1,\n",
       " 342: 3,\n",
       " 343: 4,\n",
       " 344: 1,\n",
       " 345: 1,\n",
       " 346: 2,\n",
       " 347: 4,\n",
       " 348: 4,\n",
       " 349: 1,\n",
       " 350: 4,\n",
       " 351: 4,\n",
       " 352: 1,\n",
       " 353: 1,\n",
       " 354: 2,\n",
       " 355: 4,\n",
       " 356: 4,\n",
       " 357: 3,\n",
       " 358: 4,\n",
       " 359: 4,\n",
       " 360: 0,\n",
       " 361: 1,\n",
       " 362: 1,\n",
       " 363: 1,\n",
       " 364: 1,\n",
       " 365: 4,\n",
       " 366: 1,\n",
       " 367: 0,\n",
       " 368: 4,\n",
       " 369: 0,\n",
       " 370: 3,\n",
       " 371: 4,\n",
       " 372: 3,\n",
       " 373: 1,\n",
       " 374: 1,\n",
       " 375: 4,\n",
       " 376: 2,\n",
       " 377: 1,\n",
       " 378: 1,\n",
       " 379: 1,\n",
       " 380: 0,\n",
       " 381: 1,\n",
       " 382: 3,\n",
       " 383: 3,\n",
       " 384: 3,\n",
       " 385: 1,\n",
       " 386: 3,\n",
       " 387: 2,\n",
       " 388: 1,\n",
       " 389: 1,\n",
       " 390: 1,\n",
       " 391: 2,\n",
       " 392: 0,\n",
       " 393: 2,\n",
       " 394: 1,\n",
       " 395: 1,\n",
       " 396: 4,\n",
       " 397: 0,\n",
       " 398: 2,\n",
       " 399: 4,\n",
       " 400: 3,\n",
       " 401: 4,\n",
       " 402: 0,\n",
       " 403: 3,\n",
       " 404: 2,\n",
       " 405: 4,\n",
       " 406: 4,\n",
       " 407: 1,\n",
       " 408: 2,\n",
       " 409: 4,\n",
       " 410: 3,\n",
       " 411: 3,\n",
       " 412: 0,\n",
       " 413: 4,\n",
       " 414: 1,\n",
       " 415: 4,\n",
       " 416: 2,\n",
       " 417: 3,\n",
       " 418: 4,\n",
       " 419: 1,\n",
       " 420: 0,\n",
       " 421: 1,\n",
       " 422: 0,\n",
       " 423: 3,\n",
       " 424: 1,\n",
       " 425: 2,\n",
       " 426: 0,\n",
       " 427: 0,\n",
       " 428: 2,\n",
       " 429: 2,\n",
       " 430: 4,\n",
       " 431: 1,\n",
       " 432: 3,\n",
       " 433: 1,\n",
       " 434: 1,\n",
       " 435: 4,\n",
       " 436: 0,\n",
       " 437: 4,\n",
       " 438: 1,\n",
       " 439: 1,\n",
       " 440: 3,\n",
       " 441: 3,\n",
       " 442: 1,\n",
       " 443: 3,\n",
       " 444: 3,\n",
       " 445: 1,\n",
       " 446: 0,\n",
       " 447: 0,\n",
       " 448: 2,\n",
       " 449: 3,\n",
       " 450: 4,\n",
       " 451: 1,\n",
       " 452: 4,\n",
       " 453: 2,\n",
       " 454: 0,\n",
       " 455: 0,\n",
       " 456: 1,\n",
       " 457: 0,\n",
       " 458: 4,\n",
       " 459: 2,\n",
       " 460: 4,\n",
       " 461: 1,\n",
       " 462: 0,\n",
       " 463: 1,\n",
       " 464: 0,\n",
       " 465: 4,\n",
       " 466: 1,\n",
       " 467: 0,\n",
       " 468: 4,\n",
       " 469: 1,\n",
       " 470: 0,\n",
       " 471: 0,\n",
       " 472: 4,\n",
       " 473: 3,\n",
       " 474: 4,\n",
       " 475: 3,\n",
       " 476: 2,\n",
       " 477: 2,\n",
       " 478: 0,\n",
       " 479: 3,\n",
       " 480: 1,\n",
       " 481: 4,\n",
       " 482: 2,\n",
       " 483: 4,\n",
       " 484: 4,\n",
       " 485: 1,\n",
       " 486: 3,\n",
       " 487: 4,\n",
       " 488: 2,\n",
       " 489: 4,\n",
       " 490: 2,\n",
       " 491: 2,\n",
       " 492: 3,\n",
       " 493: 3,\n",
       " 494: 2,\n",
       " 495: 1,\n",
       " 496: 2,\n",
       " 497: 4,\n",
       " 498: 4,\n",
       " 499: 1,\n",
       " 500: 0,\n",
       " 501: 1,\n",
       " 502: 3,\n",
       " 503: 3,\n",
       " 504: 0,\n",
       " 505: 3,\n",
       " 506: 2,\n",
       " 507: 3,\n",
       " 508: 1,\n",
       " 509: 4,\n",
       " 510: 4,\n",
       " 511: 0,\n",
       " 512: 2,\n",
       " 513: 2,\n",
       " 514: 2,\n",
       " 515: 1,\n",
       " 516: 3,\n",
       " 517: 3,\n",
       " 518: 1,\n",
       " 519: 1,\n",
       " 520: 1,\n",
       " 521: 2,\n",
       " 522: 1,\n",
       " 523: 1,\n",
       " 524: 3,\n",
       " 525: 3,\n",
       " 526: 4,\n",
       " 527: 0,\n",
       " 528: 1,\n",
       " 529: 2,\n",
       " 530: 4,\n",
       " 531: 4,\n",
       " 532: 3,\n",
       " 533: 1,\n",
       " 534: 4,\n",
       " 535: 1,\n",
       " 536: 4,\n",
       " 537: 4,\n",
       " 538: 2,\n",
       " 539: 4,\n",
       " 540: 2,\n",
       " 541: 1,\n",
       " 542: 3,\n",
       " 543: 0,\n",
       " 544: 1,\n",
       " 545: 0,\n",
       " 546: 0,\n",
       " 547: 2,\n",
       " 548: 0,\n",
       " 549: 1,\n",
       " 550: 0,\n",
       " 551: 2,\n",
       " 552: 0,\n",
       " 553: 1,\n",
       " 554: 3,\n",
       " 555: 4,\n",
       " 556: 2,\n",
       " 557: 4,\n",
       " 558: 3,\n",
       " 559: 1,\n",
       " 560: 4,\n",
       " 561: 0,\n",
       " 562: 4,\n",
       " 563: 2,\n",
       " 564: 0,\n",
       " 565: 2,\n",
       " 566: 3,\n",
       " 567: 3,\n",
       " 568: 1,\n",
       " 569: 0,\n",
       " 570: 1,\n",
       " 571: 1,\n",
       " 572: 1,\n",
       " 573: 4,\n",
       " 574: 2,\n",
       " 575: 2,\n",
       " 576: 2,\n",
       " 577: 0,\n",
       " 578: 2,\n",
       " 579: 0,\n",
       " 580: 0,\n",
       " 581: 3,\n",
       " 582: 2,\n",
       " 583: 2,\n",
       " 584: 1,\n",
       " 585: 4,\n",
       " 586: 0,\n",
       " 587: 3,\n",
       " 588: 0,\n",
       " 589: 1,\n",
       " 590: 4,\n",
       " 591: 1,\n",
       " 592: 0,\n",
       " 593: 1,\n",
       " 594: 3,\n",
       " 595: 2,\n",
       " 596: 1,\n",
       " 597: 1,\n",
       " 598: 1,\n",
       " 599: 1,\n",
       " 600: 3,\n",
       " 601: 1,\n",
       " 602: 4,\n",
       " 603: 1,\n",
       " 604: 3,\n",
       " 605: 3,\n",
       " 606: 0,\n",
       " 607: 1,\n",
       " 608: 1,\n",
       " 609: 4,\n",
       " 610: 1,\n",
       " 611: 3,\n",
       " 612: 1,\n",
       " 613: 2,\n",
       " 614: 1,\n",
       " 615: 0,\n",
       " 616: 2,\n",
       " 617: 2,\n",
       " 618: 4,\n",
       " 619: 3,\n",
       " 620: 4,\n",
       " 621: 2,\n",
       " 622: 1,\n",
       " 623: 3,\n",
       " 624: 0,\n",
       " 625: 0,\n",
       " 626: 0,\n",
       " 627: 4,\n",
       " 628: 1,\n",
       " 629: 4,\n",
       " 630: 4,\n",
       " 631: 3,\n",
       " 632: 2,\n",
       " 633: 0,\n",
       " 634: 4,\n",
       " 635: 4,\n",
       " 636: 0,\n",
       " 637: 1,\n",
       " 638: 1,\n",
       " 639: 1,\n",
       " 640: 0,\n",
       " 641: 4,\n",
       " 642: 0,\n",
       " 643: 1,\n",
       " 644: 2,\n",
       " 645: 2,\n",
       " 646: 3,\n",
       " 647: 2,\n",
       " 648: 1,\n",
       " 649: 2,\n",
       " 650: 2,\n",
       " 651: 2,\n",
       " 652: 1,\n",
       " 653: 3,\n",
       " 654: 4,\n",
       " 655: 1,\n",
       " 656: 1,\n",
       " 657: 2,\n",
       " 658: 1,\n",
       " 659: 2,\n",
       " 660: 1,\n",
       " 661: 0,\n",
       " 662: 1,\n",
       " 663: 1,\n",
       " 664: 3,\n",
       " 665: 2,\n",
       " 666: 2,\n",
       " 667: 1,\n",
       " 668: 3,\n",
       " 669: 3,\n",
       " 670: 4,\n",
       " 671: 4,\n",
       " 672: 1,\n",
       " 673: 0,\n",
       " 674: 1,\n",
       " 675: 1,\n",
       " 676: 1,\n",
       " 677: 0,\n",
       " 678: 1,\n",
       " 679: 4,\n",
       " 680: 4,\n",
       " 681: 1,\n",
       " 682: 1,\n",
       " 683: 0,\n",
       " 684: 2,\n",
       " 685: 4,\n",
       " 686: 1,\n",
       " 687: 1,\n",
       " 688: 2,\n",
       " 689: 3,\n",
       " 690: 1,\n",
       " 691: 3,\n",
       " 692: 1,\n",
       " 693: 2,\n",
       " 694: 4,\n",
       " 695: 3,\n",
       " 696: 4,\n",
       " 697: 1,\n",
       " 698: 2,\n",
       " 699: 1,\n",
       " 700: 3,\n",
       " 701: 3,\n",
       " 702: 0,\n",
       " 703: 4,\n",
       " 704: 2,\n",
       " 705: 4,\n",
       " 706: 1,\n",
       " 707: 1,\n",
       " 708: 3,\n",
       " 709: 3,\n",
       " 710: 2,\n",
       " 711: 3,\n",
       " 712: 2,\n",
       " 713: 1,\n",
       " 714: 1,\n",
       " 715: 1,\n",
       " 716: 4,\n",
       " 717: 3,\n",
       " 718: 3,\n",
       " 719: 4,\n",
       " 720: 4,\n",
       " 721: 1,\n",
       " 722: 2,\n",
       " 723: 1,\n",
       " 724: 1,\n",
       " 725: 4,\n",
       " 726: 4,\n",
       " 727: 4,\n",
       " 728: 0,\n",
       " 729: 0,\n",
       " 730: 0,\n",
       " 731: 3,\n",
       " 732: 3,\n",
       " 733: 4,\n",
       " 734: 2,\n",
       " 735: 1,\n",
       " 736: 0,\n",
       " 737: 3,\n",
       " 738: 4,\n",
       " 739: 2,\n",
       " 740: 4,\n",
       " 741: 4,\n",
       " 742: 1,\n",
       " 743: 1,\n",
       " 744: 2,\n",
       " 745: 3,\n",
       " 746: 2,\n",
       " 747: 3,\n",
       " 748: 3,\n",
       " 749: 1,\n",
       " 750: 1,\n",
       " 751: 4,\n",
       " 752: 3,\n",
       " 753: 4,\n",
       " 754: 3,\n",
       " 755: 1,\n",
       " 756: 3,\n",
       " 757: 0,\n",
       " 758: 3,\n",
       " 759: 0,\n",
       " 760: 4,\n",
       " 761: 2,\n",
       " 762: 0,\n",
       " 763: 1,\n",
       " 764: 2,\n",
       " 765: 3,\n",
       " 766: 3,\n",
       " 767: 1,\n",
       " 768: 2,\n",
       " 769: 3,\n",
       " 770: 3,\n",
       " 771: 1,\n",
       " 772: 3,\n",
       " 773: 4,\n",
       " 774: 1,\n",
       " 775: 3,\n",
       " 776: 4,\n",
       " 777: 2,\n",
       " 778: 4,\n",
       " 779: 1,\n",
       " 780: 0,\n",
       " 781: 2,\n",
       " 782: 1,\n",
       " 783: 0,\n",
       " 784: 4,\n",
       " 785: 1,\n",
       " 786: 4,\n",
       " 787: 1,\n",
       " 788: 1,\n",
       " 789: 0,\n",
       " 790: 0,\n",
       " 791: 4,\n",
       " 792: 1,\n",
       " 793: 4,\n",
       " 794: 3,\n",
       " 795: 3,\n",
       " 796: 1,\n",
       " 797: 4,\n",
       " 798: 3,\n",
       " 799: 1,\n",
       " 800: 0,\n",
       " 801: 1,\n",
       " 802: 0,\n",
       " 803: 1,\n",
       " 804: 4,\n",
       " 805: 1,\n",
       " 806: 3,\n",
       " 807: 0,\n",
       " 808: 4,\n",
       " 809: 3,\n",
       " 810: 4,\n",
       " 811: 3,\n",
       " 812: 0,\n",
       " 813: 4,\n",
       " 814: 2,\n",
       " 815: 3,\n",
       " 816: 0,\n",
       " 817: 0,\n",
       " 818: 1,\n",
       " 819: 2,\n",
       " 820: 2,\n",
       " 821: 2,\n",
       " 822: 1,\n",
       " 823: 0,\n",
       " 824: 4,\n",
       " 825: 4,\n",
       " 826: 0,\n",
       " 827: 3,\n",
       " 828: 1,\n",
       " 829: 1,\n",
       " 830: 1,\n",
       " 831: 1,\n",
       " 832: 1,\n",
       " 833: 3,\n",
       " 834: 3,\n",
       " 835: 1,\n",
       " 836: 3,\n",
       " 837: 2,\n",
       " 838: 3,\n",
       " 839: 3,\n",
       " 840: 4,\n",
       " 841: 2,\n",
       " 842: 1,\n",
       " 843: 1,\n",
       " 844: 1,\n",
       " 845: 2,\n",
       " 846: 4,\n",
       " 847: 0,\n",
       " 848: 4,\n",
       " 849: 1,\n",
       " 850: 1,\n",
       " 851: 0,\n",
       " 852: 3,\n",
       " 853: 4,\n",
       " 854: 4,\n",
       " 855: 2,\n",
       " 856: 1,\n",
       " 857: 3,\n",
       " 858: 1,\n",
       " 859: 4,\n",
       " 860: 1,\n",
       " 861: 2,\n",
       " 862: 0,\n",
       " 863: 4,\n",
       " 864: 4,\n",
       " 865: 4,\n",
       " 866: 3,\n",
       " 867: 2,\n",
       " 868: 1,\n",
       " 869: 4,\n",
       " 870: 2,\n",
       " 871: 0,\n",
       " 872: 1,\n",
       " 873: 2,\n",
       " 874: 0,\n",
       " 875: 3,\n",
       " 876: 1,\n",
       " 877: 4,\n",
       " 878: 0,\n",
       " 879: 1,\n",
       " 880: 1,\n",
       " 881: 4,\n",
       " 882: 1,\n",
       " 883: 1,\n",
       " 884: 0,\n",
       " 885: 3,\n",
       " 886: 2,\n",
       " 887: 3,\n",
       " 888: 4,\n",
       " 889: 1,\n",
       " 890: 1,\n",
       " 891: 1,\n",
       " 892: 1,\n",
       " 893: 2,\n",
       " 894: 4,\n",
       " 895: 1,\n",
       " 896: 3,\n",
       " 897: 1,\n",
       " 898: 1,\n",
       " 899: 4,\n",
       " 900: 4,\n",
       " 901: 4,\n",
       " 902: 2,\n",
       " 903: 2,\n",
       " 904: 2,\n",
       " 905: 2,\n",
       " 906: 4,\n",
       " 907: 4,\n",
       " 908: 1,\n",
       " 909: 1,\n",
       " 910: 0,\n",
       " 911: 4,\n",
       " 912: 3,\n",
       " 913: 4,\n",
       " 914: 0,\n",
       " 915: 1,\n",
       " 916: 4,\n",
       " 917: 4,\n",
       " 918: 1,\n",
       " 919: 0,\n",
       " 920: 2,\n",
       " 921: 2,\n",
       " 922: 3,\n",
       " 923: 2,\n",
       " 924: 1,\n",
       " 925: 4,\n",
       " 926: 4,\n",
       " 927: 1,\n",
       " 928: 3,\n",
       " 929: 1,\n",
       " 930: 1,\n",
       " 931: 1,\n",
       " 932: 0,\n",
       " 933: 1,\n",
       " 934: 1,\n",
       " 935: 0,\n",
       " 936: 1,\n",
       " 937: 3,\n",
       " 938: 3,\n",
       " 939: 3,\n",
       " 940: 4,\n",
       " 941: 3,\n",
       " 942: 3,\n",
       " 943: 2,\n",
       " 944: 4,\n",
       " 945: 1,\n",
       " 946: 1,\n",
       " 947: 1,\n",
       " 948: 4,\n",
       " 949: 3,\n",
       " 950: 3,\n",
       " 951: 1,\n",
       " 952: 4,\n",
       " 953: 1,\n",
       " 954: 0,\n",
       " 955: 1,\n",
       " 956: 4,\n",
       " 957: 1,\n",
       " 958: 1,\n",
       " 959: 3,\n",
       " 960: 2,\n",
       " 961: 3,\n",
       " 962: 4,\n",
       " 963: 4,\n",
       " 964: 0,\n",
       " 965: 1,\n",
       " 966: 4,\n",
       " 967: 3,\n",
       " 968: 3,\n",
       " 969: 0,\n",
       " 970: 4,\n",
       " 971: 0,\n",
       " 972: 2,\n",
       " 973: 0,\n",
       " 974: 2,\n",
       " 975: 2,\n",
       " 976: 1,\n",
       " 977: 0,\n",
       " 978: 4,\n",
       " 979: 4,\n",
       " 980: 1,\n",
       " 981: 1,\n",
       " 982: 1,\n",
       " 983: 1,\n",
       " 984: 4,\n",
       " 985: 1,\n",
       " 986: 4,\n",
       " 987: 1,\n",
       " 988: 4,\n",
       " 989: 2,\n",
       " 990: 0,\n",
       " 991: 0,\n",
       " 992: 2,\n",
       " 993: 3,\n",
       " 994: 1,\n",
       " 995: 0,\n",
       " 996: 3,\n",
       " 997: 1,\n",
       " 998: 3,\n",
       " 999: 3,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "submit = pd.DataFrame({'id': ids, 'flower_class': cnn_pred})\n",
    "header = [\"id\", \"flower_class\"]\n",
    "submit.to_csv('cnn_resnet_predict.csv', columns = header, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
